# ------------------------------------------------------------------------------
# Kafka:
# ------------------------------------------------------------------------------

## The StatefulSet installs 3 pods by default
replicas: 3

## The kafka image repository
image: "confluentinc/cp-kafka"

## The kafka image tag
imageTag: "5.0.1"  # Confluent image for Kafka 2.0.0

## Specify a imagePullPolicy
## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
imagePullPolicy: "IfNotPresent"

## Configure resource requests and limits
## ref: http://kubernetes.io/docs/user-guide/compute-resources/
resources: {}
  # limits:
  #   cpu: 200m
  #   memory: 1536Mi
  # requests:
  #   cpu: 100m
  #   memory: 1024Mi
kafkaHeapOptions: "-Xmx1G -Xms1G"

## The StatefulSet Update Strategy which Kafka will use when changes are applied.
## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
updateStrategy:
  type: "OnDelete"

## Start and stop pods in Parallel or OrderedReady (one-by-one.)  Note - Can not change after first release.
## ref: https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy
podManagementPolicy: OrderedReady

## Useful if using any custom authorizer
## Pass in some secrets to use (if required)
# secrets:
# - name: myKafkaSecret
#   keys:
#     - username
#     - password
#   # mountPath: /opt/kafka/secret
# - name: myZkSecret
#   keys:
#     - user
#     - pass
#   mountPath: /opt/zookeeper/secret


## The subpath within the Kafka container's PV where logs will be stored.
## This is combined with `persistence.mountPath`, to create, by default: /opt/kafka/data/logs
logSubPath: "logs"

## Use an alternate scheduler, e.g. "stork".
## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
##
# schedulerName:

## Pod scheduling preferences (by default keep pods within a release on separate nodes).
## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
## By default we don't set affinity
affinity: {}
## Alternatively, this typical example defines:
## antiAffinity (to keep Kafka pods on separate pods)
## and affinity (to encourage Kafka pods to be collocated with Zookeeper pods)
# affinity:
#   podAntiAffinity:
#     requiredDuringSchedulingIgnoredDuringExecution:
#     - labelSelector:
#         matchExpressions:
#         - key: app
#           operator: In
#           values:
#           - kafka
#       topologyKey: "kubernetes.io/hostname"
#   podAffinity:
#     preferredDuringSchedulingIgnoredDuringExecution:
#      - weight: 50
#        podAffinityTerm:
#          labelSelector:
#            matchExpressions:
#            - key: app
#              operator: In
#              values:
#                - zookeeper
#          topologyKey: "kubernetes.io/hostname"

## Node labels for pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
nodeSelector: {}

## Readiness probe config.
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/
##
readinessProbe:
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  successThreshold: 1
  failureThreshold: 3

## Period to wait for broker graceful shutdown (sigterm) before pod is killed (sigkill)
## ref: https://kubernetes-v1-4.github.io/docs/user-guide/production-pods/#lifecycle-hooks-and-termination-notice
## ref: https://kafka.apache.org/10/documentation.html#brokerconfigs controlled.shutdown.*
terminationGracePeriodSeconds: 60

# Tolerations for nodes that have taints on them.
# Useful if you want to dedicate nodes to just run kafka
# https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
tolerations: []
# tolerations:
# - key: "key"
#   operator: "Equal"
#   value: "value"
#   effect: "NoSchedule"

## Headless service.
##
headless:
  # annotations:
  # targetPort:
  port: 9092

## External access.
##
external:
  type: LoadBalancer
  # annotations:
  #  service.beta.kubernetes.io/openstack-internal-load-balancer: "true"
  dns:
    useInternal: false
    useExternal: false
  # create an A record for each statefulset pod
  distinct: false
  enabled: true
  servicePort: 19092
  firstListenerPort: 31090
  domain: cluster.local
  loadBalancerIP: 
    - ${KAFKA_IP_0}
    - ${KAFKA_IP_1}
    - ${KAFKA_IP_2}

  init:
    image: "lwolf/kubectl_deployer"
    imageTag: "0.4"
    imagePullPolicy: "IfNotPresent"

podAnnotations: {}
configurationOverrides:
  "offsets.topic.replication.factor": 3
  "confluent.support.metrics.enable": false  # Disables confluent metric submission
  "auto.leader.rebalance.enable": true
  "num.network.threads": 4
  "num.io.thread": 8
  "socket.send.buffer.bytes": "1048576"
  "socket.receive.buffer.bytes": "1048576"
  "socket.request.max.bytes": "104857600"
  "num.partitions": 8


  "advertised.listeners": EXTERNAL://${LOAD_BALANCER_IP}:31090
  "listener.security.protocol.map": PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
  "listeners": PLAINTEXT://:9092,EXTERNAL://:31090
  "inter.broker.listener.name": "PLAINTEXT"

persistence:
  enabled: true

  ## The size of the PersistentVolume to allocate to each Kafka Pod in the StatefulSet. For
  ## production servers this number should likely be much larger.
  ##
  size: "30Gi"

  ## The location within the Kafka container where the PV will mount its storage and Kafka will
  ## store its logs.
  ##
  mountPath: "/opt/kafka/data"

  ## Kafka data Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  storageClass: managed-premium

jmx:
  ## Rules to apply to the Prometheus JMX Exporter.  Note while lots of stats have been cleaned and exposed,
  ## there are still more stats to clean up and expose, others will never get exposed.  They keep lots of duplicates
  ## that can be derived easily.  The configMap in this chart cleans up the metrics it exposes to be in a Prometheus
  ## format, eg topic, broker are labels and not part of metric name. Improvements are gladly accepted and encouraged.
  configMap:

    ## Allows disabling the default configmap, note a configMap is needed
    enabled: true

    ## Allows setting values to generate confimap
    ## To allow all metrics through (warning its crazy excessive) comment out below `overrideConfig` and set
    ## `whitelistObjectNames: []`
    overrideConfig: {}
      # jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:5555/jmxrmi
      # lowercaseOutputName: true
      # lowercaseOutputLabelNames: true
      # ssl: false
      # rules:
      # - pattern: ".*"

    ## If you would like to supply your own ConfigMap for JMX metrics, supply the name of that
    ## ConfigMap as an `overrideName` here.
    overrideName: ""

  ## Port the jmx metrics are exposed in native jmx format, not in Prometheus format
  port: 5555

  ## JMX Whitelist Objects, can be set to control which JMX metrics are exposed.  Only whitelisted
  ## values will be exposed via JMX Exporter.  They must also be exposed via Rules.  To expose all metrics
  ## (warning its crazy excessive and they aren't formatted in a prometheus style) (1) `whitelistObjectNames: []`
  ## (2) commented out above `overrideConfig`.
  whitelistObjectNames:  # []
  - kafka.controller:*
  - kafka.server:*
  - java.lang:*
  - kafka.network:*
  - kafka.log:*

## Prometheus Exporters / Metrics
##
prometheus:
  ## Prometheus JMX Exporter: exposes the majority of Kafkas metrics
  jmx:
    enabled: true

    ## The image to use for the metrics collector
    image: solsson/kafka-prometheus-jmx-exporter@sha256

    ## The image tag to use for the metrics collector
    imageTag: a23062396cd5af1acdf76512632c20ea6be76885dfc20cd9ff40fb23846557e8

    ## Interval at which Prometheus scrapes metrics, note: only used by Prometheus Operator
    interval: 10s

    ## Timeout at which Prometheus timeouts scrape run, note: only used by Prometheus Operator
    scrapeTimeout: 10s

    ## Port jmx-exporter exposes Prometheus format metrics to scrape
    port: 5556

    resources: {}
      # limits:
      #   cpu: 200m
      #   memory: 1Gi
      # requests:
      #   cpu: 100m
      #   memory: 100Mi

  ## Prometheus Kafka Exporter: exposes complimentary metrics to JMX Exporter
  kafka:
    enabled: true

    ## The image to use for the metrics collector
    image: danielqsj/kafka-exporter

    ## The image tag to use for the metrics collector
    imageTag: v1.2.0

    ## Interval at which Prometheus scrapes metrics, note: only used by Prometheus Operator
    interval: 10s

    ## Timeout at which Prometheus timeouts scrape run, note: only used by Prometheus Operator
    scrapeTimeout: 10s

    ## Port kafka-exporter exposes for Prometheus to scrape metrics
    port: 9308

    ## Resource limits
    resources: {}
#      limits:
#        cpu: 200m
#        memory: 1Gi
#      requests:
#        cpu: 100m
#        memory: 100Mi

    # Tolerations for nodes that have taints on them.
    # Useful if you want to dedicate nodes to just run kafka-exporter
    # https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    tolerations: []
    # tolerations:
    # - key: "key"
    #   operator: "Equal"
    #   value: "value"
    #   effect: "NoSchedule"

    ## Pod scheduling preferences (by default keep pods within a release on separate nodes).
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## By default we don't set affinity
    affinity: {}
    ## Alternatively, this typical example defines:
    ## affinity (to encourage Kafka Exporter pods to be collocated with Kafka pods)
    # affinity:
    #   podAffinity:
    #     preferredDuringSchedulingIgnoredDuringExecution:
    #      - weight: 50
    #        podAffinityTerm:
    #          labelSelector:
    #            matchExpressions:
    #            - key: app
    #              operator: In
    #              values:
    #                - kafka
    #          topologyKey: "kubernetes.io/hostname"

    ## Node labels for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
    nodeSelector: {}

  operator:
    ## Are you using Prometheus Operator?
    enabled: false

    serviceMonitor:
      # Namespace Prometheus is installed in
      namespace: monitoring

      ## Defaults to whats used if you follow CoreOS [Prometheus Install Instructions](https://github.com/coreos/prometheus-operator/tree/master/helm#tldr)
      ## [Prometheus Selector Label](https://github.com/coreos/prometheus-operator/blob/master/helm/prometheus/templates/prometheus.yaml#L65)
      ## [Kube Prometheus Selector Label](https://github.com/coreos/prometheus-operator/blob/master/helm/kube-prometheus/values.yaml#L298)
      selector:
        prometheus: kube-prometheus

## Kafka Config job configuration
##
configJob:
  ## Specify the number of retries before considering kafka-config job as failed.
  ## https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#pod-backoff-failure-policy
  backoffLimit: 6

# ------------------------------------------------------------------------------
# Zookeeper:
# ------------------------------------------------------------------------------

zookeeper:
  ## If true, install the Zookeeper chart alongside Kafka
  ## ref: https://github.com/kubernetes/charts/tree/master/incubator/zookeeper
  enabled: true

  ## Configure Zookeeper resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  resources: ~

  replicaCount: 3

  ## Environmental variables to set in Zookeeper
  env:
    ## The JVM heap size to allocate to Zookeeper
    ZK_HEAP_SIZE: "1G"

  persistence:
    enabled: true
    # The amount of PV storage allocated to each Zookeeper pod in the statefulset
    size: "30Gi"
    storageClass: managed-premium

  ## Specify a Zookeeper imagePullPolicy
  ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  image:
    PullPolicy: "IfNotPresent"

  ## If the Zookeeper Chart is disabled a URL and port are required to connect
  url: ""
  port: 2181

  ## Pod scheduling preferences (by default keep pods within a release on separate nodes).
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## By default we don't set affinity:
  affinity: {}  # Criteria by which pod label-values influence scheduling for zookeeper pods.
  # podAntiAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     - topologyKey: "kubernetes.io/hostname"
  #       labelSelector:
  #         matchLabels:
  #           release: zookeeper
